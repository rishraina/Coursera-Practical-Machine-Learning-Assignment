---
title: "Activity Recognition Using Predictive Analysis"
output:
  html_document:
    keep_md: yes
  pdf_document: default
author: "Created By : Rishab Raina"
---
## Introduction

Using devices such as Jawbone Up, Nike Fuel Band, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerators on the belt, forearm, arm, and dumbbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: [http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har] (see the section on the Weight Lifting Exercise Data set).

For the purpose of this project, the following steps would be followed:

1. Data Loading
2. Data Cleaning & Preprocessing
2. Exploratory Data Analysis
3. Prediction Model Selection
4. Predicting Test Set Output

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading the Dataset & Libraries

The Data set has been downloaded from the internet and has been loaded into two seperate dataframes, __“training and “testing”__. The __“training__ data set has 19622 number of records and the __“testing__ data set has 20 records. The number of variables is 160.

```{r packages loading,message=FALSE}
# Package names
packages <- c("caret","corrplot","rpart","rpart.plot","RColorBrewer","RGtk2","rattle","randomForest","gbm","lubridate","dplyr")

# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# Packages loading
invisible(lapply(packages, library, character.only = TRUE))

# Loading Dataset using URL
train_url <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_url  <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"


init_training_data <- read.csv(url(train_url))
init_testing_data <- read.csv(url(test_url))


```
## Data Cleaning

First, we load the training and testing set from the online sources and then split the training set further into training and test sets. 

*Removing Variables which have nearly zero variance*
```{r data cleaning 1,message=FALSE}
non_zero_var <- nearZeroVar(init_training_data)

training_data <- init_training_data[,-non_zero_var]
testing_data <- init_testing_data[,-non_zero_var]

```
*Removing Variables which are having NA values. Our threshold is 95%.*
```{r data cleaning 2,message=FALSE}
na_val_col <- sapply(training_data, function(x) mean(is.na(x))) > 0.95

training_data <- training_data[,na_val_col == FALSE]
testing_data <- testing_data[,na_val_col == FALSE]

```
*Removing variables which will not to used for model building*
```{r data cleaning 3}
training_data <- training_data[,-(1:5)]
testing_data <- testing_data[,-(1:5)]
```
Data Cleaning Complete. As a result of data processing and cleaning, we are able to reduce the variables from 160 to 54

## EDA
Now that we have filtered out variables we are going to use in the model, we shall look at the dependence of these variables on each other through a correlation plot.
```{r CorrelationPlot, fig.width=12, fig.height=8}
library(corrplot)
corrMat <- cor(training_data[,-54])
corrplot(corrMat, method = "color", type = "lower", tl.cex = 0.8, tl.col = rgb(0,0,0))
```

### Data Partitioning
Using the training set, we will be further splitting it into 2 sets - one for training and the other test set for validation to choose the best model. That model would be chosen to run on the actual testing dataset which we left out
```{r data partition,fig.width=12, fig.height=8}
inTrain <- createDataPartition(training_data$classe, p=0.6, list=FALSE)
training <- training_data[inTrain,]
testing <- training_data[-inTrain,]

```
## Prediction Model Selection

*We will use 3 methods to model the training set and thereby choose the one having the best accuracy to predict the outcome variable in the testing set.* 
*The methods are Decision Tree, Random Forest and Generalized Boosted Model.*

Note : A confusion matrix plotted at the end of each model will help visualize the analysis better.

### Decision Tree
```{r DecisionTree, message = FALSE, warning = FALSE, fig.width=18, fig.height=10}

set.seed(7867)
modelDT <- rpart(classe ~ ., data = training, method = "class")
fancyRpartPlot(modelDT)
predictDT <- predict(modelDT, testing, type = "class")
confMatDT <- confusionMatrix(predictDT, as.factor(testing$classe))
confMatDT
```

```{r DecisionTree1,echo=FALSE,message = FALSE, warning = FALSE, fig.width=18, fig.height=10}
#Using train function in caret
# DT_modfit <- train(classe ~ ., data = training, method="rpart")
# DT_prediction <- predict(DT_modfit, testing)
# confusionMatrix(DT_prediction, testing$classe)
# rpart.plot(DT_modfit$finalModel, roundint=FALSE)
```
### Random Forest

```{r RandomForest, message = FALSE}
library(caret)
set.seed(13908)
control <- trainControl(method = "cv", number = 3, verboseIter=FALSE)
modelRF <- train(classe ~ ., data = training, method = "rf", trControl = control, ntree =100)
modelRF$finalModel
predictRF <- predict(modelRF, testing)
confMatRF <- confusionMatrix(predictRF, as.factor(testing$classe))
confMatRF

plot(confMatRF$table, col = confMatRF$byClass, 
     main = paste("Random Forest - Accuracy Level =",
                  round(confMatRF$overall['Accuracy'], 4)))
```
From the Confusion Matrix, we can clearly see that the prediction accuracy of Random Forest model is 99% which is satisfactory.

### Generalized Boosted Model

```{r GBM, message = FALSE}
library(caret)
set.seed(13908)
control <- trainControl(method = "repeatedcv", number = 5, repeats = 1, verboseIter = FALSE)
modelGBM <- train(classe ~ ., data = training, trControl = control, method = "gbm", verbose = FALSE)
modelGBM$finalModel
predictGBM <- predict(modelGBM, testing)
confMatGBM <- confusionMatrix(predictGBM, as.factor(testing$classe))
confMatGBM

plot(confMatGBM$table, col = confMatGBM$byClass, 
     main = paste("Gradient Boosting - Accuracy Level =",
                  round(confMatGBM$overall['Accuracy'], 4)))
```
From Gradient Boost Model, the prediction accuracy is 98.7% which is satisfactory.

**Now we need to see how each model has predicted the validation dataset across the classifications.** We are not considering Decision Tree model as it didn’t reach the satisfactory prediction accuracy level. So only Random Forest and Gradient Boosting methods are being compared.
As Random Forest offers the maximum accuracy of 99.6%, we will go with Random Forest Model to predict our test data class variable.

## Predicting Test Set Output

```{r TestSetPrediction, messages = FALSE}
predictRF <- predict(modelRF, testing_data)
```
### The final prediction are as follows:
```{r TestSetPrediction1,echo=FALSE}
predictRF
```

